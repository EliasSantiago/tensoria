# Deploy do Tensoria no Google Cloud Run

Este guia explica como fazer o deploy do Tensoria no Cloud Run usando a arquitetura de **Sidecar** (API + Ollama no mesmo servi√ßo) e **GCS FUSE** para persist√™ncia dos modelos.

## üìã Pr√©-requisitos

1.  Google Cloud CLI (`gcloud`) instalado e autenticado.
2.  Um projeto no Google Cloud.
3.  APIs habilitadas: Cloud Run, Cloud Build, Artifact Registry.

## üöÄ Passo a Passo

### 1. Configurar Vari√°veis

Defina as vari√°veis do seu projeto:

```bash
export PROJECT_ID="seu-projeto-id"
export REGION="us-central1"
export BUCKET_NAME="${PROJECT_ID}-tensoria-models"
export API_KEY="sua-chave-api-gerada-aqui"
```

### 2. Criar Bucket para Modelos

O Ollama precisa de um lugar persistente para salvar os modelos (que s√£o grandes). Usaremos um bucket GCS.

```bash
# Criar bucket
gcloud storage buckets create gs://${BUCKET_NAME} --location=${REGION}

# (Opcional) Configurar ciclo de vida ou classe de armazenamento se desejar economizar
```

### 3. Build e Push da Imagem da API

```bash
# Habilitar Artifact Registry API
gcloud services enable artifactregistry.googleapis.com cloudbuild.googleapis.com run.googleapis.com

# Submeter build para o Google Cloud Build
# Execute este comando na raiz do projeto 'tensoria'
gcloud builds submit --tag gcr.io/${PROJECT_ID}/tensoria-api:latest .
```

### 4. Preparar o Arquivo de Servi√ßo

O arquivo `gcp/service.yaml` precisa ter as vari√°veis substitu√≠das. Voc√™ pode usar `envsubst` ou editar manualmente.

```bash
# Usando envsubst (se dispon√≠vel)
envsubst < gcp/service.yaml > gcp/service-deployed.yaml

# OU edite manualmente gcp/service.yaml substituindo:
# ${PROJECT_ID} -> seu ID do projeto
# ${API_KEY} -> sua chave API
# ${BUCKET_NAME} -> nome do bucket criado
```

### 5. Deploy no Cloud Run

```bash
gcloud run services replace gcp/service-deployed.yaml --region ${REGION}
```

### 6. Configurar Permiss√µes

A conta de servi√ßo padr√£o do Cloud Run precisa de permiss√£o para ler/escrever no bucket.

```bash
# Obter a conta de servi√ßo padr√£o
SERVICE_ACCOUNT=$(gcloud run services describe tensoria --region ${REGION} --format 'value(spec.template.spec.serviceAccountName)')

# Se retornar vazio, √© a default compute service account:
# SERVICE_ACCOUNT="${PROJECT_ID}-compute@developer.gserviceaccount.com"

# Dar permiss√£o de Storage Admin (ou Storage Object Admin) no bucket
gcloud storage buckets add-iam-policy-binding gs://${BUCKET_NAME} \
    --member="serviceAccount:${SERVICE_ACCOUNT}" \
    --role="roles/storage.objectAdmin"
```

### 7. Instalar Modelos (Primeira Execu√ß√£o)

Como o bucket come√ßa vazio, voc√™ precisa "instalar" os modelos. Voc√™ pode fazer isso chamando a API do Ollama atrav√©s da sua API (se tiver endpoint para isso) ou, mais f√°cil, rodando um job tempor√°rio ou usando a API do Tensoria.

O Tensoria n√£o tem endpoint de "pull" exposto publicamente por seguran√ßa, mas voc√™ pode usar o comando `curl` na sua m√°quina local apontando para a URL do Cloud Run (se tiver autentica√ß√£o) ou usar um Job do Cloud Run.

**M√©todo Recomendado (Via API Tensoria):**
A API do Tensoria exp√µe endpoints do Ollama internamente? N√£o diretamente.
Voc√™ precisar√° adicionar um endpoint de "pull" na API do Tensoria ou usar um script de inicializa√ß√£o.

**Solu√ß√£o Alternativa:**
Rodar o Ollama localmente, baixar os modelos, e fazer upload da pasta `~/.ollama` para o bucket `gs://${BUCKET_NAME}`.

```bash
# Localmente
ollama pull mistral

# Copiar para o bucket
gcloud storage cp -r ~/.ollama/* gs://${BUCKET_NAME}/
```

Isso √© o mais r√°pido para popular o bucket!

## ‚ö†Ô∏è Considera√ß√µes de Performance

O Cloud Run padr√£o usa **CPU**. A infer√™ncia de LLMs em CPU pode ser lenta (alguns tokens por segundo).
Para produ√ß√£o de alta performance, considere:
1.  **Cloud Run for GPUs** (Preview - requer configura√ß√£o especial).
2.  **GKE Autopilot** com GPUs.
3.  **VM (Compute Engine)** com GPU (T4/L4) - **Recomendado para custo-benef√≠cio**.
