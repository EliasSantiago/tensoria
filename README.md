# Tensoria - Open Source LLM API Infrastructure

API de infer√™ncia de LLMs open source compat√≠vel com OpenAI, constru√≠da com FastAPI e Ollama. Fornece uma interface padronizada para executar modelos LLM open-source localmente ou em servidores dedicados.

## üéØ O que √©?

Tensoria √© uma infraestrutura de API para modelos LLM open-source que:

- **API compat√≠vel com OpenAI** - Use os mesmos endpoints que OpenAI/Gemini
- **M√∫ltiplos modelos suportados** - Mistral, DeepSeek, Qwen, Llama, e muitos outros
- **Execu√ß√£o local ou remota** - Rode modelos em sua pr√≥pria infraestrutura
- **Docker-ready** - Setup completo com Docker Compose
- **Ollama-powered** - Usa Ollama como engine de infer√™ncia
- **Sem custos de API** - Execute modelos localmente sem depender de servi√ßos externos pagos

### Casos de Uso

- Executar modelos LLM open-source sem custos de API
- Desenvolvimento e testes de aplica√ß√µes com IA
- Infraestrutura privada para modelos de IA
- Integra√ß√£o com plataformas que precisam de API compat√≠vel com OpenAI

## üèóÔ∏è Arquitetura

### Diagrama de Fluxo de Dados

```mermaid
graph TB
    subgraph "ENTRADA DE DADOS"
        CLIENT[Cliente HTTP<br/>OrkestrAI API ou Outros]
        API_REQ[Requisi√ß√µes REST<br/>OpenAI-Compatible API]
        AUTH_HEADER[Header X-API-Key<br/>Autentica√ß√£o]
    end

    subgraph "ORQUESTRA√á√ÉO"
        FASTAPI[FastAPI Router<br/>Endpoints REST]
        MIDDLEWARE[Middleware<br/>Auth, Logging]
        ROUTES[Routes<br/>/v1/chat/completions<br/>/v1/completions<br/>/v1/models]
        OLLAMA_CLIENT[Ollama Client<br/>HTTP Client para Ollama]
    end

    subgraph "IA - INFER√äNCIA"
        OLLAMA_SERVICE[Ollama Service<br/>Container Docker]
        LLM_ENGINE[LLM Engine<br/>Infer√™ncia Local]
        MODEL_LOADER[Model Loader<br/>Carregamento de Modelos]
    end

    subgraph "PERSIST√äNCIA"
        MODEL_STORAGE[(Persistent Volume<br/>Modelos Baixados)]
        CONFIG[Configura√ß√µes<br/>Vari√°veis de Ambiente]
    end

    subgraph "SA√çDA"
        STREAM_RESPONSE[Streaming Response<br/>Chunks de Texto]
        JSON_RESPONSE[Resposta JSON<br/>OpenAI-Compatible]
        ERROR_HANDLING[Error Handling<br/>Tratamento de Erros]
    end

    CLIENT -->|HTTPS| API_REQ
    API_REQ -->|X-API-Key| AUTH_HEADER
    
    AUTH_HEADER --> MIDDLEWARE
    API_REQ --> FASTAPI
    
    FASTAPI --> MIDDLEWARE
    MIDDLEWARE --> ROUTES
    
    ROUTES --> OLLAMA_CLIENT
    OLLAMA_CLIENT -->|HTTP| OLLAMA_SERVICE
    
    OLLAMA_SERVICE --> LLM_ENGINE
    LLM_ENGINE --> MODEL_LOADER
    MODEL_LOADER -->|Carregar| MODEL_STORAGE
    
    OLLAMA_SERVICE -->|Stream| STREAM_RESPONSE
    OLLAMA_SERVICE -->|JSON| JSON_RESPONSE
    
    STREAM_RESPONSE -->|SSE/JSON| CLIENT
    JSON_RESPONSE -->|JSON| CLIENT
    
    ROUTES --> ERROR_HANDLING
    ERROR_HANDLING -->|Error JSON| CLIENT
    
    CONFIG -->|Settings| FASTAPI
    CONFIG -->|Settings| OLLAMA_CLIENT
```

### Decis√µes Arquiteturais Principais

#### 1. **API Compat√≠vel com OpenAI**
- **Decis√£o:** Implementa√ß√£o de endpoints compat√≠veis com OpenAI (`/v1/chat/completions`, `/v1/completions`)
- **Motivo:** Permite uso de qualquer cliente OpenAI existente sem modifica√ß√µes
- **Impacto:** Facilita integra√ß√£o e reduz fric√ß√£o para desenvolvedores

#### 2. **Ollama como Engine de Infer√™ncia**
- **Decis√£o:** Ollama como servi√ßo de infer√™ncia em vez de implementa√ß√£o pr√≥pria
- **Motivo:** Ollama √© maduro, suporta muitos modelos e gerencia eficientemente GPU/CPU
- **Impacto:** Foco no wrapper/API em vez de infraestrutura de infer√™ncia

#### 3. **Containeriza√ß√£o com Docker Compose**
- **Decis√£o:** Separa√ß√£o em containers: API FastAPI + Ollama Service
- **Motivo:** Isolamento, escalabilidade independente e facilidade de deploy
- **Impacto:** Deploy simplificado e manuten√ß√£o mais f√°cil

#### 4. **Persistent Volume para Modelos**
- **Decis√£o:** Volume Docker persistente para armazenar modelos baixados
- **Motivo:** Modelos n√£o s√£o perdidos ao reiniciar containers
- **Impacto:** Melhor experi√™ncia de uso e economia de banda

#### 5. **Autentica√ß√£o via API Key**
- **Decis√£o:** Autentica√ß√£o simples via header `X-API-Key`
- **Motivo:** Simplicidade e compatibilidade com padr√µes de API
- **Impacto:** Seguran√ßa adequada sem complexidade desnecess√°ria

#### 6. **Streaming de Respostas**
- **Decis√£o:** Suporte a streaming de respostas (SSE ou JSON streaming)
- **Motivo:** Experi√™ncia de usu√°rio melhor com feedback em tempo real
- **Impacto:** Reduz percep√ß√£o de lat√™ncia, especialmente para modelos maiores

#### 7. **Nginx como Reverse Proxy (Produ√ß√£o)**
- **Decis√£o:** Nginx na frente da API em produ√ß√£o com IP allowlist
- **Motivo:** Seguran√ßa adicional e controle de acesso
- **Impacto:** Camada extra de prote√ß√£o para ambientes de produ√ß√£o

#### 8. **Modelos N√£o Baixados Automaticamente**
- **Decis√£o:** Modelos devem ser baixados manualmente via `ollama pull`
- **Motivo:** Controle sobre espa√ßo em disco e escolha de modelos
- **Impacto:** Flexibilidade para o usu√°rio escolher modelos adequados ao seu caso

#### 9. **Timeout e Keep-Alive Configur√°veis**
- **Decis√£o:** Vari√°veis de ambiente para controlar timeouts e keep-alive
- **Motivo:** Flexibilidade para diferentes cen√°rios (modelos r√°pidos vs lentos)
- **Impacto:** Melhor adapta√ß√£o a diferentes modelos e hardware

#### 10. **Desabilita√ß√£o de Docs em Produ√ß√£o**
- **Decis√£o:** Endpoints `/docs`, `/redoc` desabilitados quando `API_KEY` est√° configurada
- **Motivo:** Seguran√ßa - n√£o expor documenta√ß√£o em produ√ß√£o
- **Impacto:** Redu√ß√£o de superf√≠cie de ataque

## üöÄ Quick Start

### Pr√©-requisitos

- **Docker** e **Docker Compose** instalados
- **Espa√ßo em disco** suficiente (cada modelo pode ter de 2GB a 40GB+)

### Passo a Passo Completo

```bash
# 1. Clonar reposit√≥rio
git clone https://github.com/seu-usuario/tensoria.git
cd tensoria

# 2. Subir os containers (Ollama + API)
docker compose up -d

# 3. Aguardar inicializa√ß√£o (alguns segundos)
sleep 10

# 4. Verificar se est√° rodando
curl http://localhost:8002/health
# Deve retornar: {"status": "ok"}

# 5. Baixar um modelo (veja se√ß√£o abaixo para detalhes)
docker exec -it tensoria-ollama ollama pull mistral

# 6. Verificar modelos instalados
curl http://localhost:8002/v1/models
```

> ‚ö†Ô∏è **IMPORTANTE**: Nenhum modelo √© baixado automaticamente!

### 2. Instalar modelos manualmente

‚ö†Ô∏è **IMPORTANTE**: Nenhum modelo √© baixado automaticamente! Voc√™ precisa baixar manualmente os modelos que deseja usar.

#### Como baixar um modelo espec√≠fico

```bash
# Formato b√°sico
docker exec -it tensoria-ollama ollama pull <nome-do-modelo>

# Exemplos:

# Mistral (recomendado para come√ßar - ~4GB)
docker exec -it tensoria-ollama ollama pull mistral

# Mistral com vers√£o espec√≠fica
docker exec -it tensoria-ollama ollama pull mistral:7b-instruct

# DeepSeek Coder (bom para c√≥digo - ~4GB)
docker exec -it tensoria-ollama ollama pull deepseek-coder:6.7b

# DeepSeek Coder vers√£o maior (melhor qualidade - ~20GB)
docker exec -it tensoria-ollama ollama pull deepseek-coder:33b

# Qwen (bom custo-benef√≠cio - ~4GB)
docker exec -it tensoria-ollama ollama pull qwen:7b

# Qwen vers√£o mais recente
docker exec -it tensoria-ollama ollama pull qwen2:7b

# Llama 3 (Meta - ~4GB)
docker exec -it tensoria-ollama ollama pull llama3

# Llama 3 vers√£o maior (8B - ~5GB)
docker exec -it tensoria-ollama ollama pull llama3:8b

# CodeLlama (especializado em c√≥digo - ~4GB)
docker exec -it tensoria-ollama ollama pull codellama

# Phi-3 (Microsoft - pequeno e eficiente - ~2GB)
docker exec -it tensoria-ollama ollama pull phi3
```

#### Verificar modelos instalados

```bash
# Listar todos os modelos baixados
docker exec -it tensoria-ollama ollama list

# Exemplo de sa√≠da:
# NAME                    ID              SIZE    MODIFIED
# mistral:latest          abc123def456    4.1 GB  2 hours ago
# deepseek-coder:6.7b     def456ghi789    4.2 GB  1 hour ago
```

#### Remover um modelo

```bash
# Remover um modelo espec√≠fico para liberar espa√ßo
docker exec -it tensoria-ollama ollama rm mistral

# Verificar espa√ßo liberado
docker exec -it tensoria-ollama ollama list
```

#### Baixar modelo com tag espec√≠fica

Alguns modelos t√™m m√∫ltiplas vers√µes/tags dispon√≠veis:

```bash
# Ver todas as tags dispon√≠veis de um modelo
docker exec -it tensoria-ollama ollama show mistral

# Baixar vers√£o espec√≠fica
docker exec -it tensoria-ollama ollama pull mistral:7b-instruct-q4_0
docker exec -it tensoria-ollama ollama pull mistral:7b-instruct-q8_0
```

#### Modelos recomendados por caso de uso

**Para come√ßar (pequeno e r√°pido):**
```bash
docker exec -it tensoria-ollama ollama pull mistral          # ~4GB
docker exec -it tensoria-ollama ollama pull phi3            # ~2GB
```

**Para c√≥digo:**
```bash
docker exec -it tensoria-ollama ollama pull deepseek-coder:6.7b  # ~4GB
docker exec -it tensoria-ollama ollama pull codellama            # ~4GB
```

**Para qualidade m√°xima:**
```bash
docker exec -it tensoria-ollama ollama pull deepseek-coder:33b   # ~20GB
docker exec -it tensoria-ollama ollama pull llama3:70b            # ~40GB
```

**Para multil√≠ngue (portugu√™s):**
```bash
docker exec -it tensoria-ollama ollama pull qwen:7b              # ~4GB
docker exec -it tensoria-ollama ollama pull qwen2:7b             # ~4GB
```

### 3. Verificar status

```bash
# Health check
curl http://localhost:8002/health

# Listar modelos instalados
curl http://localhost:8002/v1/models
```

## üì° Endpoints da API

### Chat Completions (OpenAI-compatible)

```bash
curl -X POST http://localhost:8002/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [
      {"role": "user", "content": "Ol√°, como vai?"}
    ],
    "temperature": 0.7
  }'
```

### Text Completions (Legacy)

```bash
curl -X POST http://localhost:8002/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "prompt": "O c√©u √©",
    "max_tokens": 50
  }'
```

### List Models

```bash
curl http://localhost:8002/v1/models
```

### Health Check

```bash
curl http://localhost:8002/health
```

## üîí Seguran√ßa

O Tensoria implementa m√∫ltiplas camadas de seguran√ßa:

### 1. API Key Authentication

Todas as requisi√ß√µes (exceto `/health`) requerem o header `X-API-Key`:

```bash
# Gerar uma API Key segura
python -c "import secrets; print(f'tensoria_{secrets.token_urlsafe(48)}')"

# Configurar no .env do servidor
API_KEY=tensoria_sua_chave_aqui
```

**Exemplo de requisi√ß√£o autenticada:**

```bash
curl -X POST https://tensoria.orkestrai.com.br/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-API-Key: tensoria_sua_chave_aqui" \
  -d '{"model": "mistral", "messages": [{"role": "user", "content": "Ol√°!"}]}'
```

### 2. IP Allowlist (Nginx)

O Nginx est√° configurado para aceitar apenas requisi√ß√µes do servidor orkestrai-api:

```nginx
allow 34.42.168.19;  # orkestrai-api
deny all;
```

### 3. Documenta√ß√£o Desabilitada em Produ√ß√£o

Quando `API_KEY` est√° configurada, os endpoints `/docs`, `/redoc` e `/openapi.json` s√£o desabilitados automaticamente.

## ‚öôÔ∏è Configura√ß√£o

Vari√°veis de ambiente dispon√≠veis (arquivo `.env`):

```env
# Seguran√ßa (OBRIGAT√ìRIO em produ√ß√£o!)
API_KEY=tensoria_sua_chave_aqui

# Portas
OLLAMA_PORT=11434
API_PORT=8002

# Ollama
OLLAMA_TIMEOUT=120
OLLAMA_KEEP_ALIVE=5m
OLLAMA_NUM_PARALLEL=1

# API
LOG_LEVEL=INFO
DEFAULT_MODEL=mistral
MAX_TOKENS=4096
DEFAULT_TEMPERATURE=0.7
```

## ü§ñ Modelos Suportados

| Modelo | Tamanho | Uso Recomendado |
|--------|---------|-----------------|
| `mistral` | ~4GB | Uso geral, chat |
| `mistral:7b-instruct` | ~4GB | Instru√ß√µes precisas |
| `deepseek-coder` | ~4GB | C√≥digo |
| `deepseek-coder:6.7b` | ~4GB | C√≥digo (equilibrado) |
| `deepseek-coder:33b` | ~20GB | C√≥digo (alta qualidade) |
| `qwen:7b` | ~4GB | Uso geral, multil√≠ngue |
| `qwen2:7b` | ~4GB | Vers√£o mais recente |

## üîß Comandos √öteis

### Gerenciamento de Containers

```bash
# Ver logs da API
docker compose logs -f api

# Ver logs do Ollama
docker compose logs -f ollama

# Ver logs de ambos
docker compose logs -f

# Parar tudo
docker compose down

# Parar e remover volumes (‚ö†Ô∏è apaga modelos baixados!)
docker compose down -v

# Reiniciar servi√ßos
docker compose restart

# Rebuild ap√≥s mudan√ßas no c√≥digo
docker compose up -d --build
```

### Gerenciamento de Modelos

```bash
# Listar modelos instalados
docker exec -it tensoria-ollama ollama list

# Baixar um modelo espec√≠fico
docker exec -it tensoria-ollama ollama pull <nome-do-modelo>

# Ver informa√ß√µes de um modelo
docker exec -it tensoria-ollama ollama show <nome-do-modelo>

# Remover um modelo espec√≠fico
docker exec -it tensoria-ollama ollama rm <nome-do-modelo>

# Verificar espa√ßo usado pelos modelos
docker exec -it tensoria-ollama du -sh /root/.ollama/models
```

### Testar Modelos

```bash
# Testar um modelo diretamente no Ollama
docker exec -it tensoria-ollama ollama run mistral "Ol√°, como voc√™ est√°?"

# Testar via API Tensoria
curl -X POST http://localhost:8002/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [{"role": "user", "content": "Ol√°!"}]
  }'
```

## üîÆ Prepara√ß√£o para Futuro

Esta arquitetura foi projetada para permitir:

- [ ] Roteamento inteligente de modelos
- [ ] Fallback entre modelos
- [ ] Integra√ß√£o com LiteLLM
- [ ] Uso como provider interno do OrkestrAI
- [ ] Escalonamento por GPU/VRAM

## üìÅ Estrutura do Projeto

```
tensoria/
‚îú‚îÄ‚îÄ docker-compose.yml    # Configura√ß√£o dos servi√ßos
‚îú‚îÄ‚îÄ Dockerfile            # Build da API
‚îú‚îÄ‚îÄ requirements.txt      # Depend√™ncias Python
‚îú‚îÄ‚îÄ README.md             # Esta documenta√ß√£o
‚îî‚îÄ‚îÄ api/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ main.py           # Entry point FastAPI
    ‚îú‚îÄ‚îÄ config.py         # Configura√ß√µes
    ‚îú‚îÄ‚îÄ models.py         # Schemas Pydantic
    ‚îú‚îÄ‚îÄ ollama_client.py  # Cliente HTTP Ollama
    ‚îî‚îÄ‚îÄ routes/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ chat.py       # /v1/chat/completions
        ‚îú‚îÄ‚îÄ completions.py # /v1/completions
        ‚îú‚îÄ‚îÄ models.py     # /v1/models
        ‚îî‚îÄ‚îÄ health.py     # /health
```

## üõ°Ô∏è Produ√ß√£o

Para deploy em GCP:

1. Configurar GPU (se dispon√≠vel)
2. Ajustar vari√°veis de ambiente
3. Configurar reverse proxy (nginx)
4. Implementar autentica√ß√£o
5. Configurar monitoramento

## üìù Exemplo Completo de Uso

### 1. Subir os servi√ßos

```bash
cd tensoria
docker compose up -d

# Aguardar inicializa√ß√£o
sleep 10

# Verificar status
curl http://localhost:8002/health
```

### 2. Baixar um modelo

```bash
# Baixar Mistral (recomendado para come√ßar)
docker exec -it tensoria-ollama ollama pull mistral

# Verificar se foi baixado
docker exec -it tensoria-ollama ollama list
```

### 3. Testar a API

```bash
# Listar modelos dispon√≠veis
curl http://localhost:8002/v1/models

# Fazer uma requisi√ß√£o de chat
curl -X POST http://localhost:8002/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [
      {"role": "user", "content": "Explique o que √© Python em uma frase"}
    ],
    "temperature": 0.7
  }'
```

### 4. Integrar com outras aplica√ß√µes

A API √© compat√≠vel com OpenAI, ent√£o voc√™ pode usar qualquer cliente OpenAI:

```python
# Exemplo Python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8002/v1",
    api_key="not-needed"  # Ou sua API_KEY se configurada
)

response = client.chat.completions.create(
    model="mistral",
    messages=[{"role": "user", "content": "Ol√°!"}]
)

print(response.choices[0].message.content)
```

