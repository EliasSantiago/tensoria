# Tensoria - Open Source LLM API Infrastructure

API de inferÃªncia de LLMs open source para a plataforma OrkestrAI.

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Tensoria                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â–¶ â”‚  FastAPI Backend (API)     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  - /v1/chat/completions    â”‚   â”‚
â”‚                           â”‚  - /v1/completions         â”‚   â”‚
â”‚                           â”‚  - /v1/models              â”‚   â”‚
â”‚                           â”‚  - /health                 â”‚   â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                       â”‚                      â”‚
â”‚                                       â–¼                      â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                           â”‚       Ollama Service        â”‚   â”‚
â”‚                           â”‚  (LLM Inference Engine)     â”‚   â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                       â”‚                      â”‚
â”‚                                       â–¼                      â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                           â”‚   Persistent Volume         â”‚   â”‚
â”‚                           â”‚   (Downloaded Models)       â”‚   â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Subir os containers

```bash
cd tensoria
docker compose up -d
```

> âš ï¸ **IMPORTANTE**: Nenhum modelo Ã© baixado automaticamente!

### 2. Instalar modelos manualmente

```bash
# Mistral (recomendado para comeÃ§ar - ~4GB)
docker exec -it tensoria-ollama ollama pull mistral

# DeepSeek Coder (bom para cÃ³digo)
docker exec -it tensoria-ollama ollama pull deepseek-coder:6.7b

# Qwen (bom custo-benefÃ­cio)
docker exec -it tensoria-ollama ollama pull qwen:7b
```

### 3. Verificar status

```bash
# Health check
curl http://localhost:8002/health

# Listar modelos instalados
curl http://localhost:8002/v1/models
```

## ğŸ“¡ Endpoints da API

### Chat Completions (OpenAI-compatible)

```bash
curl -X POST http://localhost:8002/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [
      {"role": "user", "content": "OlÃ¡, como vai?"}
    ],
    "temperature": 0.7
  }'
```

### Text Completions (Legacy)

```bash
curl -X POST http://localhost:8002/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "prompt": "O cÃ©u Ã©",
    "max_tokens": 50
  }'
```

### List Models

```bash
curl http://localhost:8002/v1/models
```

### Health Check

```bash
curl http://localhost:8002/health
```

## ğŸ”’ SeguranÃ§a

O Tensoria implementa mÃºltiplas camadas de seguranÃ§a:

### 1. API Key Authentication

Todas as requisiÃ§Ãµes (exceto `/health`) requerem o header `X-API-Key`:

```bash
# Gerar uma API Key segura
python -c "import secrets; print(f'tensoria_{secrets.token_urlsafe(48)}')"

# Configurar no .env do servidor
API_KEY=tensoria_sua_chave_aqui
```

**Exemplo de requisiÃ§Ã£o autenticada:**

```bash
curl -X POST https://tensoria.orkestrai.com.br/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-API-Key: tensoria_sua_chave_aqui" \
  -d '{"model": "mistral", "messages": [{"role": "user", "content": "OlÃ¡!"}]}'
```

### 2. IP Allowlist (Nginx)

O Nginx estÃ¡ configurado para aceitar apenas requisiÃ§Ãµes do servidor orkestrai-api:

```nginx
allow 34.42.168.19;  # orkestrai-api
deny all;
```

### 3. DocumentaÃ§Ã£o Desabilitada em ProduÃ§Ã£o

Quando `API_KEY` estÃ¡ configurada, os endpoints `/docs`, `/redoc` e `/openapi.json` sÃ£o desabilitados automaticamente.

## âš™ï¸ ConfiguraÃ§Ã£o

VariÃ¡veis de ambiente disponÃ­veis (arquivo `.env`):

```env
# SeguranÃ§a (OBRIGATÃ“RIO em produÃ§Ã£o!)
API_KEY=tensoria_sua_chave_aqui

# Portas
OLLAMA_PORT=11434
API_PORT=8002

# Ollama
OLLAMA_TIMEOUT=120
OLLAMA_KEEP_ALIVE=5m
OLLAMA_NUM_PARALLEL=1

# API
LOG_LEVEL=INFO
DEFAULT_MODEL=mistral
MAX_TOKENS=4096
DEFAULT_TEMPERATURE=0.7
```

## ğŸ¤– Modelos Suportados

| Modelo | Tamanho | Uso Recomendado |
|--------|---------|-----------------|
| `mistral` | ~4GB | Uso geral, chat |
| `mistral:7b-instruct` | ~4GB | InstruÃ§Ãµes precisas |
| `deepseek-coder` | ~4GB | CÃ³digo |
| `deepseek-coder:6.7b` | ~4GB | CÃ³digo (equilibrado) |
| `deepseek-coder:33b` | ~20GB | CÃ³digo (alta qualidade) |
| `qwen:7b` | ~4GB | Uso geral, multilÃ­ngue |
| `qwen2:7b` | ~4GB | VersÃ£o mais recente |

## ğŸ”§ Comandos Ãšteis

```bash
# Ver logs da API
docker compose logs -f api

# Ver logs do Ollama
docker compose logs -f ollama

# Parar tudo
docker compose down

# Remover volumes (apaga modelos baixados)
docker compose down -v

# Ver modelos instalados no Ollama
docker exec -it tensoria-ollama ollama list

# Remover um modelo especÃ­fico
docker exec -it tensoria-ollama ollama rm mistral
```

## ğŸ”® PreparaÃ§Ã£o para Futuro

Esta arquitetura foi projetada para permitir:

- [ ] Roteamento inteligente de modelos
- [ ] Fallback entre modelos
- [ ] IntegraÃ§Ã£o com LiteLLM
- [ ] Uso como provider interno do OrkestrAI
- [ ] Escalonamento por GPU/VRAM

## ğŸ“ Estrutura do Projeto

```
tensoria/
â”œâ”€â”€ docker-compose.yml    # ConfiguraÃ§Ã£o dos serviÃ§os
â”œâ”€â”€ Dockerfile            # Build da API
â”œâ”€â”€ requirements.txt      # DependÃªncias Python
â”œâ”€â”€ README.md             # Esta documentaÃ§Ã£o
â””â”€â”€ api/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ main.py           # Entry point FastAPI
    â”œâ”€â”€ config.py         # ConfiguraÃ§Ãµes
    â”œâ”€â”€ models.py         # Schemas Pydantic
    â”œâ”€â”€ ollama_client.py  # Cliente HTTP Ollama
    â””â”€â”€ routes/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ chat.py       # /v1/chat/completions
        â”œâ”€â”€ completions.py # /v1/completions
        â”œâ”€â”€ models.py     # /v1/models
        â””â”€â”€ health.py     # /health
```

## ğŸ›¡ï¸ ProduÃ§Ã£o

Para deploy em GCP:

1. Configurar GPU (se disponÃ­vel)
2. Ajustar variÃ¡veis de ambiente
3. Configurar reverse proxy (nginx)
4. Implementar autenticaÃ§Ã£o
5. Configurar monitoramento

---

**OrkestrAI** - Infraestrutura de IA Open Source
