# Tensoria - Open Source LLM API Infrastructure

API de infer√™ncia de LLMs open source compat√≠vel com OpenAI, constru√≠da com FastAPI e Ollama. Fornece uma interface padronizada para executar modelos LLM open-source localmente ou em servidores dedicados.

## üéØ O que √©?

Tensoria √© uma infraestrutura de API para modelos LLM open-source que:

- **API compat√≠vel com OpenAI** - Use os mesmos endpoints que OpenAI/Gemini
- **M√∫ltiplos modelos suportados** - Mistral, DeepSeek, Qwen, Llama, e muitos outros
- **Execu√ß√£o local ou remota** - Rode modelos em sua pr√≥pria infraestrutura
- **Docker-ready** - Setup completo com Docker Compose
- **Ollama-powered** - Usa Ollama como engine de infer√™ncia
- **Sem custos de API** - Execute modelos localmente sem depender de servi√ßos externos pagos

### Casos de Uso

- Executar modelos LLM open-source sem custos de API
- Desenvolvimento e testes de aplica√ß√µes com IA
- Infraestrutura privada para modelos de IA
- Integra√ß√£o com plataformas que precisam de API compat√≠vel com OpenAI

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Tensoria                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                              ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ   Client    ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  FastAPI Backend (API)     ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  - /v1/chat/completions    ‚îÇ   ‚îÇ
‚îÇ                           ‚îÇ  - /v1/completions         ‚îÇ   ‚îÇ
‚îÇ                           ‚îÇ  - /v1/models              ‚îÇ   ‚îÇ
‚îÇ                           ‚îÇ  - /health                 ‚îÇ   ‚îÇ
‚îÇ                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                       ‚îÇ                      ‚îÇ
‚îÇ                                       ‚ñº                      ‚îÇ
‚îÇ                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ                           ‚îÇ       Ollama Service        ‚îÇ   ‚îÇ
‚îÇ                           ‚îÇ  (LLM Inference Engine)     ‚îÇ   ‚îÇ
‚îÇ                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                       ‚îÇ                      ‚îÇ
‚îÇ                                       ‚ñº                      ‚îÇ
‚îÇ                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ                           ‚îÇ   Persistent Volume         ‚îÇ   ‚îÇ
‚îÇ                           ‚îÇ   (Downloaded Models)       ‚îÇ   ‚îÇ
‚îÇ                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Quick Start

### Pr√©-requisitos

- **Docker** e **Docker Compose** instalados
- **Espa√ßo em disco** suficiente (cada modelo pode ter de 2GB a 40GB+)

### Passo a Passo Completo

```bash
# 1. Clonar reposit√≥rio
git clone https://github.com/seu-usuario/tensoria.git
cd tensoria

# 2. Subir os containers (Ollama + API)
docker compose up -d

# 3. Aguardar inicializa√ß√£o (alguns segundos)
sleep 10

# 4. Verificar se est√° rodando
curl http://localhost:8002/health
# Deve retornar: {"status": "ok"}

# 5. Baixar um modelo (veja se√ß√£o abaixo para detalhes)
docker exec -it tensoria-ollama ollama pull mistral

# 6. Verificar modelos instalados
curl http://localhost:8002/v1/models
```

> ‚ö†Ô∏è **IMPORTANTE**: Nenhum modelo √© baixado automaticamente!

### 2. Instalar modelos manualmente

‚ö†Ô∏è **IMPORTANTE**: Nenhum modelo √© baixado automaticamente! Voc√™ precisa baixar manualmente os modelos que deseja usar.

#### Como baixar um modelo espec√≠fico

```bash
# Formato b√°sico
docker exec -it tensoria-ollama ollama pull <nome-do-modelo>

# Exemplos:

# Mistral (recomendado para come√ßar - ~4GB)
docker exec -it tensoria-ollama ollama pull mistral

# Mistral com vers√£o espec√≠fica
docker exec -it tensoria-ollama ollama pull mistral:7b-instruct

# DeepSeek Coder (bom para c√≥digo - ~4GB)
docker exec -it tensoria-ollama ollama pull deepseek-coder:6.7b

# DeepSeek Coder vers√£o maior (melhor qualidade - ~20GB)
docker exec -it tensoria-ollama ollama pull deepseek-coder:33b

# Qwen (bom custo-benef√≠cio - ~4GB)
docker exec -it tensoria-ollama ollama pull qwen:7b

# Qwen vers√£o mais recente
docker exec -it tensoria-ollama ollama pull qwen2:7b

# Llama 3 (Meta - ~4GB)
docker exec -it tensoria-ollama ollama pull llama3

# Llama 3 vers√£o maior (8B - ~5GB)
docker exec -it tensoria-ollama ollama pull llama3:8b

# CodeLlama (especializado em c√≥digo - ~4GB)
docker exec -it tensoria-ollama ollama pull codellama

# Phi-3 (Microsoft - pequeno e eficiente - ~2GB)
docker exec -it tensoria-ollama ollama pull phi3
```

#### Verificar modelos instalados

```bash
# Listar todos os modelos baixados
docker exec -it tensoria-ollama ollama list

# Exemplo de sa√≠da:
# NAME                    ID              SIZE    MODIFIED
# mistral:latest          abc123def456    4.1 GB  2 hours ago
# deepseek-coder:6.7b     def456ghi789    4.2 GB  1 hour ago
```

#### Remover um modelo

```bash
# Remover um modelo espec√≠fico para liberar espa√ßo
docker exec -it tensoria-ollama ollama rm mistral

# Verificar espa√ßo liberado
docker exec -it tensoria-ollama ollama list
```

#### Baixar modelo com tag espec√≠fica

Alguns modelos t√™m m√∫ltiplas vers√µes/tags dispon√≠veis:

```bash
# Ver todas as tags dispon√≠veis de um modelo
docker exec -it tensoria-ollama ollama show mistral

# Baixar vers√£o espec√≠fica
docker exec -it tensoria-ollama ollama pull mistral:7b-instruct-q4_0
docker exec -it tensoria-ollama ollama pull mistral:7b-instruct-q8_0
```

#### Modelos recomendados por caso de uso

**Para come√ßar (pequeno e r√°pido):**
```bash
docker exec -it tensoria-ollama ollama pull mistral          # ~4GB
docker exec -it tensoria-ollama ollama pull phi3            # ~2GB
```

**Para c√≥digo:**
```bash
docker exec -it tensoria-ollama ollama pull deepseek-coder:6.7b  # ~4GB
docker exec -it tensoria-ollama ollama pull codellama            # ~4GB
```

**Para qualidade m√°xima:**
```bash
docker exec -it tensoria-ollama ollama pull deepseek-coder:33b   # ~20GB
docker exec -it tensoria-ollama ollama pull llama3:70b            # ~40GB
```

**Para multil√≠ngue (portugu√™s):**
```bash
docker exec -it tensoria-ollama ollama pull qwen:7b              # ~4GB
docker exec -it tensoria-ollama ollama pull qwen2:7b             # ~4GB
```

### 3. Verificar status

```bash
# Health check
curl http://localhost:8002/health

# Listar modelos instalados
curl http://localhost:8002/v1/models
```

## üì° Endpoints da API

### Chat Completions (OpenAI-compatible)

```bash
curl -X POST http://localhost:8002/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [
      {"role": "user", "content": "Ol√°, como vai?"}
    ],
    "temperature": 0.7
  }'
```

### Text Completions (Legacy)

```bash
curl -X POST http://localhost:8002/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "prompt": "O c√©u √©",
    "max_tokens": 50
  }'
```

### List Models

```bash
curl http://localhost:8002/v1/models
```

### Health Check

```bash
curl http://localhost:8002/health
```

## üîí Seguran√ßa

O Tensoria implementa m√∫ltiplas camadas de seguran√ßa:

### 1. API Key Authentication

Todas as requisi√ß√µes (exceto `/health`) requerem o header `X-API-Key`:

```bash
# Gerar uma API Key segura
python -c "import secrets; print(f'tensoria_{secrets.token_urlsafe(48)}')"

# Configurar no .env do servidor
API_KEY=tensoria_sua_chave_aqui
```

**Exemplo de requisi√ß√£o autenticada:**

```bash
curl -X POST https://tensoria.orkestrai.com.br/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "X-API-Key: tensoria_sua_chave_aqui" \
  -d '{"model": "mistral", "messages": [{"role": "user", "content": "Ol√°!"}]}'
```

### 2. IP Allowlist (Nginx)

O Nginx est√° configurado para aceitar apenas requisi√ß√µes do servidor orkestrai-api:

```nginx
allow 34.42.168.19;  # orkestrai-api
deny all;
```

### 3. Documenta√ß√£o Desabilitada em Produ√ß√£o

Quando `API_KEY` est√° configurada, os endpoints `/docs`, `/redoc` e `/openapi.json` s√£o desabilitados automaticamente.

## ‚öôÔ∏è Configura√ß√£o

Vari√°veis de ambiente dispon√≠veis (arquivo `.env`):

```env
# Seguran√ßa (OBRIGAT√ìRIO em produ√ß√£o!)
API_KEY=tensoria_sua_chave_aqui

# Portas
OLLAMA_PORT=11434
API_PORT=8002

# Ollama
OLLAMA_TIMEOUT=120
OLLAMA_KEEP_ALIVE=5m
OLLAMA_NUM_PARALLEL=1

# API
LOG_LEVEL=INFO
DEFAULT_MODEL=mistral
MAX_TOKENS=4096
DEFAULT_TEMPERATURE=0.7
```

## ü§ñ Modelos Suportados

| Modelo | Tamanho | Uso Recomendado |
|--------|---------|-----------------|
| `mistral` | ~4GB | Uso geral, chat |
| `mistral:7b-instruct` | ~4GB | Instru√ß√µes precisas |
| `deepseek-coder` | ~4GB | C√≥digo |
| `deepseek-coder:6.7b` | ~4GB | C√≥digo (equilibrado) |
| `deepseek-coder:33b` | ~20GB | C√≥digo (alta qualidade) |
| `qwen:7b` | ~4GB | Uso geral, multil√≠ngue |
| `qwen2:7b` | ~4GB | Vers√£o mais recente |

## üîß Comandos √öteis

### Gerenciamento de Containers

```bash
# Ver logs da API
docker compose logs -f api

# Ver logs do Ollama
docker compose logs -f ollama

# Ver logs de ambos
docker compose logs -f

# Parar tudo
docker compose down

# Parar e remover volumes (‚ö†Ô∏è apaga modelos baixados!)
docker compose down -v

# Reiniciar servi√ßos
docker compose restart

# Rebuild ap√≥s mudan√ßas no c√≥digo
docker compose up -d --build
```

### Gerenciamento de Modelos

```bash
# Listar modelos instalados
docker exec -it tensoria-ollama ollama list

# Baixar um modelo espec√≠fico
docker exec -it tensoria-ollama ollama pull <nome-do-modelo>

# Ver informa√ß√µes de um modelo
docker exec -it tensoria-ollama ollama show <nome-do-modelo>

# Remover um modelo espec√≠fico
docker exec -it tensoria-ollama ollama rm <nome-do-modelo>

# Verificar espa√ßo usado pelos modelos
docker exec -it tensoria-ollama du -sh /root/.ollama/models
```

### Testar Modelos

```bash
# Testar um modelo diretamente no Ollama
docker exec -it tensoria-ollama ollama run mistral "Ol√°, como voc√™ est√°?"

# Testar via API Tensoria
curl -X POST http://localhost:8002/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [{"role": "user", "content": "Ol√°!"}]
  }'
```

## üîÆ Prepara√ß√£o para Futuro

Esta arquitetura foi projetada para permitir:

- [ ] Roteamento inteligente de modelos
- [ ] Fallback entre modelos
- [ ] Integra√ß√£o com LiteLLM
- [ ] Uso como provider interno do OrkestrAI
- [ ] Escalonamento por GPU/VRAM

## üìÅ Estrutura do Projeto

```
tensoria/
‚îú‚îÄ‚îÄ docker-compose.yml    # Configura√ß√£o dos servi√ßos
‚îú‚îÄ‚îÄ Dockerfile            # Build da API
‚îú‚îÄ‚îÄ requirements.txt      # Depend√™ncias Python
‚îú‚îÄ‚îÄ README.md             # Esta documenta√ß√£o
‚îî‚îÄ‚îÄ api/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ main.py           # Entry point FastAPI
    ‚îú‚îÄ‚îÄ config.py         # Configura√ß√µes
    ‚îú‚îÄ‚îÄ models.py         # Schemas Pydantic
    ‚îú‚îÄ‚îÄ ollama_client.py  # Cliente HTTP Ollama
    ‚îî‚îÄ‚îÄ routes/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ chat.py       # /v1/chat/completions
        ‚îú‚îÄ‚îÄ completions.py # /v1/completions
        ‚îú‚îÄ‚îÄ models.py     # /v1/models
        ‚îî‚îÄ‚îÄ health.py     # /health
```

## üõ°Ô∏è Produ√ß√£o

Para deploy em GCP:

1. Configurar GPU (se dispon√≠vel)
2. Ajustar vari√°veis de ambiente
3. Configurar reverse proxy (nginx)
4. Implementar autentica√ß√£o
5. Configurar monitoramento

## üìù Exemplo Completo de Uso

### 1. Subir os servi√ßos

```bash
cd tensoria
docker compose up -d

# Aguardar inicializa√ß√£o
sleep 10

# Verificar status
curl http://localhost:8002/health
```

### 2. Baixar um modelo

```bash
# Baixar Mistral (recomendado para come√ßar)
docker exec -it tensoria-ollama ollama pull mistral

# Verificar se foi baixado
docker exec -it tensoria-ollama ollama list
```

### 3. Testar a API

```bash
# Listar modelos dispon√≠veis
curl http://localhost:8002/v1/models

# Fazer uma requisi√ß√£o de chat
curl -X POST http://localhost:8002/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "messages": [
      {"role": "user", "content": "Explique o que √© Python em uma frase"}
    ],
    "temperature": 0.7
  }'
```

### 4. Integrar com outras aplica√ß√µes

A API √© compat√≠vel com OpenAI, ent√£o voc√™ pode usar qualquer cliente OpenAI:

```python
# Exemplo Python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8002/v1",
    api_key="not-needed"  # Ou sua API_KEY se configurada
)

response = client.chat.completions.create(
    model="mistral",
    messages=[{"role": "user", "content": "Ol√°!"}]
)

print(response.choices[0].message.content)
```

---

**Tensoria** - Infraestrutura de IA Open Source

Desenvolvido com ‚ù§Ô∏è usando Python, FastAPI, Ollama e Docker
